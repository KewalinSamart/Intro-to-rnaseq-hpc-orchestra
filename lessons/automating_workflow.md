---
title: "Automating an RNA-Seq workflow"
author: "Radhika Khetani, Bob Freeman, Meeta Mistry"
date: "Wednesday, July 6, 2016"
---
Approximate time: 2 hours

## Learning Objectives:

* Automate the whole RNA-Seq workflow using a shell script
* Learn commands that make it even more flexible

## Automating a workflow

Let's step back and revisit the process of getting from fastqc to getting a count matrix...

The easiest way to keep track of and repeat the process is to capture the steps that we've performed in a bash script. We already have 2 separate scripts for parts of this workflow, one LSF submission script that performs the fastqc and a regular shell script using positional parameters that runs STAR. In this module we are going to make a new script that combines these 2 steps with featureCounts, and learn a few new things along the way.

***Before we get started, please log into Orchestra and start a new interactive session with 1 core.***

### Granting our workflow even more flexibility with a couple of new commands

We already have a good understanding of positional parameters, vectors within shell scripts, and understand the value of commenting liberally for the sake of your future self (and others who might use your script). Today, we will be putting the workflow together using 2 new commands: 

**1.** `basename`

This command will remove the full path of the file and just leave behind the filename, thus making our script more versatile. In addition, this command can make file names shorter. Let's try this out:

```bash
$ basename ~/ngs_course/rnaseq/raw_data/Mov10_oe_1.subset.fq
```

What if we only wanted it to return the name of the file, but without the extension .fq?

```bash
$ basename ~/ngs_course/rnaseq/raw_data/Mov10_oe_1.subset.fq .fq
```

What if we only wanted it to return the name of the sample?

```bash
$ basename ~/ngs_course/rnaseq/raw_data/Mov10_oe_1.subset.fq .subset.fq	
```

If you wanted to store the output of this command in a variable, you can write it as follows:

```bash
$ base=$(basename ~/ngs_course/rnaseq/raw_data/Mov10_oe_1.subset.fq .subset.fq)
$ echo $base
```

**2.** `set`

This command is essentially a debugging tool (`set -x`) that will display the command before executing it. In case of an issue with the commands in the shell script, this type of debugging lets you quickly pinpoint the step that is throwing an error. This is useful in the case where the tool is not explicitly stated in the error message, or if the error message is unclear about which tool it was created by. 
	
```bash
$ set -x
$ ls -lhtr ~/ngs_course/rnaseq/raw_data/
	
$ set +x  # turns it off
```

### Granting our Workflow even More Flexibility

Several changes need to be made to the last 2 scripts we made used for fastqc and alignment respectively, so let's start by writing a new script with excerpts from the older ones. 

```bash
$ cd ~/ngs_course/rnaseq/scripts/

$ cat mov10_fastqc.lsf
$ cat star_analysis_on_input_file.sh
```

I find it easier to write a longer script in a text editor on my computer, and I suggest you do the same for this session.

Let's start with the basics of the shell script. When you are writing a multi-step workflow that accepts command-line options (positional parameters), it is very important to have the usage right in the beginning of the script. In addition to the usage, it is a good practice to put in brief comments about the inputs, steps/tools and the expected output. It is easier to fill both of these in after your script is ready and you have done a test run. So even though this is at the top of the script, it may be the last thing you add to the script.

Using the debugging command `set -x` is helpful, as alluded to before, when the tools/commands are not verbose when they run or when they fail. However, in our case all the tools are verbose and I find that the extra lines generated by `set -x` in the standard output are a distraction, and I have commented it out. *However, please note that this is a personal preference and you should try it for yourself and decide your preference.*

```bash
#!/bin/bash

# USAGE: sh rnaseq_analysis_on_input_file.sh <fastq files> <number of cores> <path and name of output directory>
# This script will take a fastq file, number of cores (for STAR), and a path to the new analysis directory as input. It will perform the following steps on the fastq file and save all results to the specified directory.
	## starting with fastqc, 
	## followed by splice-aware alignment with STAR, 
	## generation of counts associated with genes using featureCounts.

# debugging with set -x [OPTIONAL]

# set -x
```

The next step is to make sure that the command-line input is stored in variables with names that make sense to you. In addition we are going to save the name of the sample (output of the `basename` command) into a new variable that we will be using to name the output files and directories. 

```bash
# assign the command line input to new variables
fq=$1
cores=$2
output_dir=$3

# shorten the name of the file

fname=$(basename $fq .fq)
```

We need to add a few more commands to set up our environment: 
1. Setting up the environment to run all the tools we want to run 
2. Making all the sub-directories to store the outputs of the tools
Having short messages at various stages of the script is a helpful to keep track of how far along the script is.

```bash
echo "****Running rnaseq analysis on $fname****"

# Loading all the modules and adding featureCounts to the PATH

module load seq/STAR/2.5.3a
module load seq/fastqc/0.11.3

export PATH=/opt/bcbio/local/bin:$PATH

# make all of our output directories
	## The -p option means mkdir will create the whole path if it does not exist, and refrain from complaining if it does exist

mkdir -p $output_dir/STAR_alignment $output_dir/counts $output_dir/fastqc $output_dir/multiqc
```

Next, we need to define some variables that store information that is likely to change more readily. These give your script a lot of versatility and enable quick modifications. 

```bash
# define variables to make modifications easier

# genome and gtf files that are likely to change
genome=/groups/hbctraining/ngs-data-analysis2016/rnaseq/reference_data/reference_STAR 
gtf=~/ngs_course/rnaseq/reference_data/chr1-hg19_genes.gtf

# output of alignment
align_out_prefix=$output_dir/STAR_alignment/${fname}_

# input and output of counts
bam_file=$output_dir/STAR_alignment/${fname}_Aligned.sortedByCoord.out.bam
counts=$output_dir/counts/${fname}.counts
```

Let's start with the FastQC evaluation of the input fastq file.

```bash
# FastQC 
echo "****FastQC****"
fastqc $fq
```

Next, set up to run STAR. 

```bash
# Alignment with STAR

echo "****Running STAR alignment****"

STAR --runThreadN $cores \
--genomeDir $genome \
--readFilesIn $fq \
--outFileNamePrefix $align_out_prefix \
--outFilterMultimapNmax 10 \
--outReadsUnmapped Fastx \
--outSAMtype BAM SortedByCoordinate \
--outSAMunmapped Within \
--outSAMattributes Standard 
```

Finally, run featureCounts on the one alignment file that is generated. The featureCounts output has only 2 columns we're interested in, so we use the `awk` command to extract those 2 columns.

```bash
# Counting reads with featureCounts

echo "****Running featureCounts****"

featureCounts -T $cores -s 2 -a $gtf -o $counts $bam_file
awk '{print $1"\t"$7}' $counts > $counts.txt
```

Now that we have created the script on our text editor, let's copy it over to the cluster.

```bash
$ pwd		# check that you are in the `/home/eCommonsID/ngs_course/rnaseq/scripts/` directory

$ vim rnaseq_analysis_on_input_file.sh
```

Next, we want to create a submission script that has a `for` loop which will run the above script on all the fastq files in a given directory, *in parallel* like we did for the script we used to run R. You can see the progress of the jobs submitted to LSF by using the `bjobs` command (note that there is a lag of about 60 seconds between what is happening and what is reported). Don't forget about the `bkill` command, should something go wrong and you need to cancel your jobs.
	
```bash
$ vim submission_loop.sh
```

```bash
#!/bin/bash

for fq in ~/ngs_course/rnaseq/raw_data/*.fq
do
base=$(basename $fq .subset.fq)
bsub -q mcore -n 6 -W 1:30 -R "rusage[mem=4000]" -J rnaseq_mov10.$base -o %J.out -e %J.err "sh ~/ngs_course/rnaseq/scripts/rnaseq_analysis_on_input_file.sh $fq 6 ~/ngs_course/rnaseq/new_analysis"
sleep 1
done
```

> NOTE: All job schedulers are similar, but not the same. Once you understand how one works, you can transition to another one without too much trouble. They all have their pros and cons that the system administrators for your setup have taken into consideration and picked one that fits the needs of the users best. 

Once all your jobs are completed, there are 3 additional steps you will have to perform manually:

(1) Merge all the counts files to generate a count matrix.

You can do this using `paste` and `awk` together as follows:

```bash
## testing
$ paste ../new_analysis/counts/*.txt | head   # check that gene names are matching for all samples
$ paste ../new_analysis/counts/*.txt | awk '{print $1"\t"$2"\t"$4"\t"$6"\t"$8"\t"$10"\t"$12"\t"$14"\t"$16}' | head   # check that you are extracting the columns you want to extract
	
## the final command
$ paste ../new_analysis/counts/*.txt | awk '{print $1"\t"$2"\t"$4"\t"$6"\t"$8"\t"$10"\t"$12"\t"$14"\t"$16}' > ../new_analysis/counts/all_counts.txt 		
```

> **A better and faster alternative to running featureCounts for each file in the script, would be just run featurecounts on all the bam files together, once all the jobs have completed.**

(2) Move the fastQC output into the `$output_dir/fastqc` directory

(3) Run multiQC on the final outputs from fastqc, STAR and featureCounts

## Using R on a Unix system

You can also run R scripts from the command prompt in Unix. These scripts are just like shell scripts, but with R code in them; we created a few last session. For running a script from the Unix command prompt, it will have to take into account the absolute or relative location of the files and folders that will be used. Also, your local environment will need to have all the packages installed and available. 

You can run an R script from the shell command prompt in several ways, 3 different ways are listed below for a script called `mean.R`:
**Do not run this**
	
```bash
$ R < mean.R
	
$ R CMD BATCH mean.R
	
$ Rscript mean.R
```

### R on Orchestra:

R is available on Orchestra, and you can do all of the things we did on our laptops on the cluster instead. Let's try this out:

```bash
$ module avail stats/R
	
$ module load stats/R/3.2.5
	
$ R
```

As you can see, various versions of R are available on Orchestra, but there is no RStudio-like GUI. You can quit R and get back to the `$` command prompt by typing `q()`, no need to save the workspace image.
	
You can use any of the above ways to run an Rscript on Orchestra. But, you will need a different shebang line:

```bash
#!/usr/bin/env Rscript
```
And, you can also submit it as a job to the LSF queue as follows:

```bash
$ bsub -q short -W 12:00 -R "rusage[mem=16000]" "Rscript mean.R" 
# note the high memory usage above
```

Talk to the folks at HMS RC to find out which packages are already installed, and also about the best way to install R packages locally. They have a [how-to guide available online](https://wiki.med.harvard.edu/Orchestra/PersonalRPackages) for installing packages locally, if you feel comfortable trying it on your own.

***

*This lesson has been developed by members of the teaching team at the [Harvard Chan Bioinformatics Core (HBC)](http://bioinformatics.sph.harvard.edu/). These are open access materials distributed under the terms of the [Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/) (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.*

* *The materials used in this lesson was derived from work that is Copyright © Data Carpentry (http://datacarpentry.org/). 
All Data Carpentry instructional material is made available under the [Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/) (CC BY 4.0).*
